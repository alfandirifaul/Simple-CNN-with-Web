{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. Import necessary libraries",
   "id": "342370ffec1a0378"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-08T15:48:19.754189Z",
     "start_time": "2025-08-08T15:48:16.586279Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "print(\"Import necessary libraries\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import yaml\n",
    "import warnings"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import necessary libraries\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. Configure settings",
   "id": "dce080cbbb9ebb65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T15:48:28.604487Z",
     "start_time": "2025-08-08T15:48:28.597963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First of all read parameters from configuration file\n",
    "assert os.path.exists('../configuration.yaml'), \"Configuration file 'configuration.yaml' not found.\"\n",
    "with open('../configuration.yaml') as file:\n",
    "    parameters = yaml.safe_load(file)\n",
    "\n",
    "# Check the parameters\n",
    "assert parameters['batchSize'] > 0, \"Batch size must be greater than 0.\"\n",
    "assert parameters['imgHeight'] == parameters['imgWidth'], \"Image height and width must be equal.\"\n",
    "assert parameters['trainDatasetPath'] is not None, \"Train dataset path must be specified.\"\n",
    "assert parameters['epochs'] > 0, \"Number of epochs must be greater than 0.\"\n",
    "\n",
    "# If everything is ok, print the parameters\n",
    "print(\"[INFO] Configuration parameters...\")\n",
    "print(\"* Batch size........................:\", parameters['batchSize'])\n",
    "print(\"* Image height......................:\", parameters['imgHeight'])\n",
    "print(\"* Image width.......................:\", parameters['imgWidth'])\n",
    "print(\"* Train dataset path................:\", parameters['trainDatasetPath'])\n",
    "print(\"* Number of epochs..................:\", parameters['epochs'])\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=UserWarning\n",
    ")"
   ],
   "id": "6bf4c464f0568630",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Configuration parameters...\n",
      "* Batch size........................: 32\n",
      "* Image height......................: 180\n",
      "* Image width.......................: 180\n",
      "* Train dataset path................: data/train\n",
      "* Number of epochs..................: 50\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3. Create necessary functions",
   "id": "f345bdfa9075e32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T15:48:32.255498Z",
     "start_time": "2025-08-08T15:48:32.249562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to load the dataset\n",
    "def loadDataset(path, batchsz, imgHeight, imgWidth):\n",
    "    print(\"[INFO] Loading dataset...\")\n",
    "\n",
    "    trainDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        path,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=123,\n",
    "        image_size=(imgHeight, imgWidth),\n",
    "        batch_size=batchsz\n",
    "    )\n",
    "\n",
    "    validDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        path,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=123,\n",
    "        image_size=(imgHeight, imgWidth),\n",
    "        batch_size=batchsz\n",
    "    )\n",
    "\n",
    "    # Get the class names and number of classes\n",
    "    classNames = trainDataset.class_names\n",
    "    numClasses = len(classNames)\n",
    "\n",
    "    print(\"[INFO] Number of training samples:\", len(trainDataset))\n",
    "    print(\"[INFO] Class names:\", classNames)\n",
    "\n",
    "    # Cache and prefetch the datasets for performance and shuffling dataset sort\n",
    "    autotune = tf.data.AUTOTUNE\n",
    "    trainDataset.cache().shuffle(1000).prefetch(buffer_size=autotune)\n",
    "    validDataset.cache().prefetch(buffer_size=autotune)\n",
    "\n",
    "    # Return the datasets, class names, and number of classes\n",
    "    return trainDataset, validDataset, classNames, numClasses\n",
    "\n",
    "\n"
   ],
   "id": "fb2abb26b0d5ad54",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to build the model\n",
    "def buildModel(imgWidth, imgHeight, numClasses):\n",
    "    print(\"[INFO] Building model...\")\n",
    "\n",
    "    # Define the data augmentation layer and the model architecture\n",
    "    dataAugmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip('horizontal',\n",
    "                                   input_shape=(imgHeight, imgWidth, 3)\n",
    "                                   ),\n",
    "        tf.keras.layers.RandomRotation(0.1),\n",
    "        tf.keras.layers.RandomZoom(0.1)\n",
    "\n",
    "    ])\n",
    "\n",
    "    # Build the model\n",
    "    model = tf.keras.Sequential([\n",
    "        dataAugmentation,\n",
    "        tf.keras.layers.Rescaling(1. / 255),\n",
    "        tf.keras.layers.Conv2D(16,\n",
    "                               3,\n",
    "                               padding='same',\n",
    "                               activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(32,\n",
    "                               3,\n",
    "                               padding='same',\n",
    "                               activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(64,\n",
    "                               3,\n",
    "                               padding='same',\n",
    "                               activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128,\n",
    "                              activation='relu'),\n",
    "        tf.keras.layers.Dense(numClasses,\n",
    "                              activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model"
   ],
   "id": "70bae5c2539885c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to compile the model\n",
    "def compileModel(model):\n",
    "    print(\"[INFO] Compiling model...\")\n",
    "\n",
    "    # Compile the model with the Adam optimizer, sparse categorical crossentropy loss, and accuracy metric\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=['acc']\n",
    "    )\n",
    "\n",
    "    # Print the model summary\n",
    "    model.summary()"
   ],
   "id": "48bfe6ceb1ec5b2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to train the model\n",
    "def trainModel(model, epochs, trainDataset, validDataset):\n",
    "    print(\"[INFO] Training model...\")\n",
    "\n",
    "    # Train the model using the training dataset and validate it using the validation dataset\n",
    "    history = model.fit(\n",
    "        trainDataset,\n",
    "        validation_data=validDataset,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "\n",
    "    return history"
   ],
   "id": "4109713ee4f23bba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4. Visualize training history",
   "id": "e4c8169131aef689"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# function to visualize training history\n",
    "def visualizeTrainingHistory(history, epochs):\n",
    "    print(\"[INFO] Visualizing training history...\")\n",
    "\n",
    "    # Extract accuracy and loss from the history\n",
    "    acc = history.history['acc']\n",
    "    valAcc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    valLoss = history.history['val_loss']\n",
    "    epochsRange = range(epochs)\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochsRange, acc, label='Akurasi Training')\n",
    "    plt.plot(epochsRange, valAcc, label='Akurasi Validasi')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Akurasi Training dan Validasi')\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochsRange, loss, label='Loss Training')\n",
    "    plt.plot(epochsRange, valLoss, label='Loss Validasi')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Loss Training dan Validasi')\n",
    "\n",
    "    # Show the plots\n",
    "    plt.show()"
   ],
   "id": "4fe8841193a2fbf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5. Define the main function for classification",
   "id": "3787e5584d3e6b12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to perform classification\n",
    "def classification(path, batchSize, imgHeight, imgWidth, epochs):\n",
    "    # 1. Load the dataset\n",
    "    trainDataset, validDataset, classNames, numClasses = loadDataset(\n",
    "        path, batchSize, imgHeight, imgWidth\n",
    "    )\n",
    "\n",
    "    # 2. Build the model\n",
    "    model = buildModel(\n",
    "        imgWidth, imgHeight, numClasses\n",
    "    )\n",
    "\n",
    "    # 3. Compile the model\n",
    "    compileModel(model)\n",
    "\n",
    "    # 4. Train the model\n",
    "    history = trainModel(\n",
    "        model, epochs, trainDataset, validDataset\n",
    "    )\n",
    "\n",
    "    # 5. Save the model\n",
    "    model.save('classification_model.keras')\n",
    "\n",
    "    # 6. Visualize the training history\n",
    "    visualizeTrainingHistory(history, epochs)\n"
   ],
   "id": "bdac0b8144e568d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check the directory of the training dataset\n",
    "if not os.path.exists(parameters['trainDatasetPath']):\n",
    "    print(f\"[ERROR] Training dataset path '{parameters['trainDatasetPath']}' does not exist.\")\n",
    "else:\n",
    "    classification(\n",
    "        parameters['trainDatasetPath'],\n",
    "        parameters['batchSize'],\n",
    "        parameters['imgHeight'],\n",
    "        parameters['imgWidth'],\n",
    "        parameters['epochs']\n",
    "    )"
   ],
   "id": "b35f4635836983f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ffc3975dd874a0b6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
